
import re
from collections import defaultdict
from random import choices 
import math

class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns a model from a given text.
        It supoprts language generation and the evaluation of a given string.
        The class can be applied on both word level and caracter level.
    """

    
    def __init__(self, n=3, chars=False):
        """Initializing a language model object.
        Arges:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.n_tokens=n
        self.chars=chars

    def build_model(self, text):  #should be called build_model
        """populates a dictionary counting all ngrams in the specified text.

            Args:
                text (str): the text to construct the model from.
        """
        
        concat_str='' if self.chars else ' '
        n=self.n_tokens
        self.lang_model = defaultdict(int)
        self.lang_model_ngrams = defaultdict(int)
        self.vocab = defaultdict(int)
        tokens=tokenize(text,self.chars)
        ngrams=zip(*[tokens[i:] for i in range(n)])
        for key in [ngram for ngram in ngrams]:
            self.lang_model[concat_str.join(key)]+=1
            self.lang_model_ngrams[key]+=1
        #    [' '.join(ngram) for ngram in ngrams ]
        for token in tokens:
            self.vocab[token]+=1

    def get_model(self):
        """Returns the model as a dictionary of the form {ngram:count}
        """
        return self.lang_model
    
    def get_model_ngrams(self):
        """Returns the model as a dictionary of the form {(ngram_0,ngram_1...,ngram_i):count}
        """
        return self.lang_model_ngrams
    
    def next_token(self,context):
        '''
        return the next word by models' distribution
        Args:
            context: the context of the text that serach in the model
        Returns:
            (str) next for that genarate by the model distribution
        '''
        context=fix_context_length(context,self.n_tokens,self.chars)
        text_dict=defaultdict(int)
        token_context=tokenize(context,self.chars)
        ln=len(token_context)

        if ln==0:
            return None
        
        if ln==1:
            keys , values = zip(*self.vocab.items())
            return choices(keys,values)[0]
            
        for key,val in self.lang_model_ngrams.items():
            if tuple(token_context) == key[:ln]:

                text_dict[key[ln]]+=val

        if not text_dict:
            return None

        keys , values = zip(*text_dict.items())
        return choices(keys,values)[0]
    
    
    def sample_context(self):
        '''
        return a context that sampled from the models' contexts distribution.
        Args:
            n_tokens: number of tokens
        Returns:
            str. The text for context from the model distribution.
        '''
        total=0
        li_text=[]
        li_val=[]
        for key,val in self.lang_model.items():
            total+=val
            li_val.append(val)
            li_text.append(key)
        li_val = [val/total for val in li_val]
        words=choices(li_text,li_val)[0]
        return fix_context_length(words,self.n_tokens)
   

    def generate(self, context=None, n=20, stop=True):
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context should be sampled
        from the models' contexts distribution. Generation should stop before the n'th word if the
        contexts are exhausted.

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.
        """
        concat_str='' if self.chars else ' '
        if not context:
           context = self.sample_context()#sample context from model
        context = normalize_text(context,chars=self.chars)#normilize context
        new_text = context
        for i in range(n-self.n_tokens-1):
            t = self.next_token(context)
            if not t:
                return new_text
            else:
                new_text=new_text+concat_str+t
                context=fix_context_length(context+concat_str+t,self.n_tokens,self.chars)
        return new_text
    
    def get_count_ngrams(self,token_context):
        '''
        return the numerator and the denominator of token_context count in the model
        
        Args:
            token_context (str or list of str) the text for evaluation.
        
        Returns:
            list, list of ngram tokenize that was counted 
            int, count of ngram
            int, count of ngram-1
        '''
        total=0
        if type(token_context) is str:
            token_context = tokenize(token_context,self.chars)
        n=min(len(token_context),self.n_tokens)
        dic= defaultdict(int)
        
        for key,val in self.lang_model_ngrams.items():
            
            if tuple(token_context)[:n-1] == key[:n-1]:
                if tuple(token_context)[:n] == key[:n]:
                    dic[tuple(token_context)[:n]]+=val
                else:
                    dic[tuple(token_context)[:n]]+=0
                total+=val
                    
        return tuple(token_context)[:n],dic[tuple(token_context)[:n]],total
            
    
    def evaluate(self,text):
        """Returns the log-likelihod of the specified text to be generated by the model.
           Laplace smoothing should be applied if necessary.

           Args:
               text (str): Text to ebaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        concat_str='' if self.chars else ' '
        tokens=tokenize(text,self.chars)
        value=0
        for i, _ in enumerate(tokens):
            if i<self.n_tokens:
               token , numerator , denominator = self.get_count_ngrams(tokens[:i+1])
            else:
               token , numerator , denominator = self.get_count_ngrams(tokens[i-self.n_tokens+1:i+1])
                
            if numerator == 0 or denominator == 0:
                prob = self.smooth(concat_str.join(token))
            else:
                prob = numerator / denominator
            value += math.log(prob)
        return value
    
    
    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """
        tokens=tokenize(ngram,self.chars)
        _ , numerator , denominator = self.get_count_ngrams(tokens)    
        
        return (numerator+1)/(denominator+len(self.vocab))
            
        

def fix_context_length(context,n_tokens,chars=False):
    '''
    return the context after fix is number of tokens in case the cotext dosent fit the model ngram length
    Args:
        context(str): context of text
        n_tokens(int): number of tokens
    
    returns:
        str. The context that fit for the tokens length
    '''
    concat_str=''
    if not chars:
        concat_str=' '
    tokens=tokenize(context,chars)
    context = concat_str.join(tokens[-(n_tokens-1):])
    return context


#explain: 1. lower case of all characters. It will remain less characters that is the same in understanding the sentance
#         2. Add spaces between punctuation simbols for better separting between words. but it can change by parametrs(for hashtag # for example)
#         3. Remove punctuation exlude '.'. Those punctuation may not contribute to the sentance. but it can change by parametrs(for hashtag # for example)
#         4. Remove extra spaces in a row and leave only one space between tokens
#         5. Multi dots in a row switch to 'dots'(str only in word normilze and not characters normilize)
def normalize_text(text,pad_punc='!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~',remove_punc='!"#$%&\'()*+,-/:;<=>?@[\\]^_`{|}~',chars=False):
    """Returns a normalized string based on the specifiy string.
       You can add default parameters as you like (they should have default values!)
       You should explain your decitions in the header of the function.

       Args:
           text (str): the text to normalize
           pad_punc(str): characters for creating a space before and after the characters (working only when normilize word and not characters)
                           default: all string punctuation
           remove_punc(str): characters to remove from the text
                           default: all string punctuation without '.'
           chars(boolean): True - normilize characters. False - normalize words
                           default: False
       Returns:
           string. the normalized text.
    """
    punc_spaces = re.compile('([%s])' % re.escape(pad_punc))
    punc = re.compile('[%s]' % re.escape(remove_punc))
    text = text.lower()
    if not chars:
        text = re.sub('\.{3,}',' dots',text)
        text = re.sub(punc_spaces, r' \1 ', text)
    text = re.sub(punc,'',text)
    text = re.sub('\s{2,}', ' ', text)
    if not chars:
        text=text.strip()
    return text


def tokenize(text,chars=False):
    """
    Return list of tokens(text saparate) based on specific string and tokenize level
    Args:
        text(str): The text for tokenize
        chars(boolean): True - tokenize in character level . False - tokenize by space
                        default - False
    Returns:
        list contain a strings
    """
    if not chars:
        text=text.split(" ")
    return [token for token in text if token not in [""]] 
    

